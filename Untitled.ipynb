{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc02d64-f901-412f-8636-9bacd3f40375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhnh/python_venv/cv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from repvit_sam import SamPredictor, sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afec0dc-f42a-423a-b4e6-a99f47966d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = \"/home/minhnh/project_drive/CV/FewshotObjectDetection/VoxDet-simplified/repvit_sam.pt\"\n",
    "model_type = \"repvit\"\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "repvit_sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "repvit_sam = repvit_sam.to(device=device)\n",
    "repvit_sam.eval()\n",
    "\n",
    "def rpn_sam(filepath):\n",
    "    # scale_factor = img_metas['scale_factor']\n",
    "    image = Image.open(filepath)\n",
    "    w, h = image.size\n",
    "\n",
    "    xvalues = np.linspace(0, w, 25, dtype='int')\n",
    "    yvalues = np.linspace(0, h, 25, dtype='int')\n",
    "    xx, yy = np.meshgrid(xvalues, yvalues)\n",
    "    positions = np.column_stack([xx.ravel(), yy.ravel()]).astype(int)\n",
    "\n",
    "    predictor = SamPredictor(repvit_sam)\n",
    "    nd_image = np.array(image)\n",
    "    predictor.set_image(nd_image)\n",
    "    point_label = np.array([1])\n",
    "    prompt_points = np.expand_dims(positions, 1)\n",
    "\n",
    "    proposals = []\n",
    "    for point in prompt_points:\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=point,\n",
    "            point_labels=point_label,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        proposals.append(masks[0])\n",
    "    bounding_boxes = torch.zeros((len(proposals), 4), dtype=torch.float)\n",
    "    for index, mask in enumerate(proposals):\n",
    "        mask = torch.Tensor(mask)\n",
    "        h, w = mask.shape\n",
    "        if (mask==0).all():\n",
    "            bounding_boxes[index, 0] = w//2 - 2\n",
    "            bounding_boxes[index, 1] = h//2 - 2\n",
    "            bounding_boxes[index, 2] = w//2 + 2\n",
    "            bounding_boxes[index, 3] = h//2 + 2\n",
    "        else:\n",
    "            y, x = torch.where(mask != 0)\n",
    "            bounding_boxes[index, 0] = torch.min(x)\n",
    "            bounding_boxes[index, 1] = torch.min(y)\n",
    "            bounding_boxes[index, 2] = torch.max(x)\n",
    "            bounding_boxes[index, 3] = torch.max(y)\n",
    "    # bounding_boxes *= scale_factor\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3f4f68-0f29-4400-9dc5-3b394f228633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/5000 [00:29<5:55:59,  4.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     img_id \u001b[38;5;241m=\u001b[39m img_filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrpn_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimgs_base_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimg_filename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproposals_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mimg_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mrpn_sam\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     25\u001b[0m proposals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m prompt_points:\n\u001b[0;32m---> 27\u001b[0m     masks, scores, logits \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoint_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoint_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoint_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     proposals\u001b[38;5;241m.\u001b[39mappend(masks[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     33\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(proposals), \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "File \u001b[0;32m/home/minhnh/project_drive/CV/FewshotObjectDetection/VoxDet-simplified/repvit_sam/predictor.py:155\u001b[0m, in \u001b[0;36mSamPredictor.predict\u001b[0;34m(self, point_coords, point_labels, box, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    152\u001b[0m     mask_input_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mask_input, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    153\u001b[0m     mask_input_torch \u001b[38;5;241m=\u001b[39m mask_input_torch[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m--> 155\u001b[0m masks, iou_predictions, low_res_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_input_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m masks_np \u001b[38;5;241m=\u001b[39m masks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    165\u001b[0m iou_predictions_np \u001b[38;5;241m=\u001b[39m iou_predictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/home/minhnh/python_venv/cv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/minhnh/project_drive/CV/FewshotObjectDetection/VoxDet-simplified/repvit_sam/predictor.py:223\u001b[0m, in \u001b[0;36mSamPredictor.predict_torch\u001b[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    220\u001b[0m     points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Embed prompts\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Predict masks\u001b[39;00m\n\u001b[1;32m    230\u001b[0m low_res_masks, iou_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmask_decoder(\n\u001b[1;32m    231\u001b[0m     image_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,\n\u001b[1;32m    232\u001b[0m     image_pe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprompt_encoder\u001b[38;5;241m.\u001b[39mget_dense_pe(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     multimask_output\u001b[38;5;241m=\u001b[39mmultimask_output,\n\u001b[1;32m    236\u001b[0m )\n",
      "File \u001b[0;32m/home/minhnh/python_venv/cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/minhnh/python_venv/cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/minhnh/project_drive/CV/FewshotObjectDetection/VoxDet-simplified/repvit_sam/modeling/prompt_encoder.py:155\u001b[0m, in \u001b[0;36mPromptEncoder.forward\u001b[0;34m(self, points, boxes, masks)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     coords, labels \u001b[38;5;241m=\u001b[39m points\n\u001b[0;32m--> 155\u001b[0m     point_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     sparse_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sparse_embeddings, point_embeddings], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/minhnh/project_drive/CV/FewshotObjectDetection/VoxDet-simplified/repvit_sam/modeling/prompt_encoder.py:88\u001b[0m, in \u001b[0;36mPromptEncoder._embed_points\u001b[0;34m(self, points, labels, pad)\u001b[0m\n\u001b[1;32m     86\u001b[0m point_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe_layer\u001b[38;5;241m.\u001b[39mforward_with_coords(points, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_image_size)\n\u001b[1;32m     87\u001b[0m point_embedding[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[43mpoint_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_a_point_embed\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m     89\u001b[0m point_embedding[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_embeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m     90\u001b[0m point_embedding[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_embeddings[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prefix = '/home/minhnh/project_drive/CV/FewshotObjectDetection/data/OWID//P2/'\n",
    "imgs_base_path = prefix + 'images/'\n",
    "proposals_base_path = prefix + 'proposals/'\n",
    "img_filepath = os.listdir(imgs_base_path)\n",
    "img_filepath = sorted(img_filepath)\n",
    "start_idx = 0\n",
    "end_idx = 5000\n",
    "for img_filename in tqdm(img_filepath[start_idx:end_idx]):\n",
    "    with torch.no_grad():\n",
    "        img_id = img_filename.split('.')[0]\n",
    "\n",
    "        output = rpn_sam(f'{imgs_base_path}{img_filename}')\n",
    "        np.save(f'{proposals_base_path}{img_id}.npy', output.cpu().numpy())\n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
